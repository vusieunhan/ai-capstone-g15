{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5321de05",
   "metadata": {
    "papermill": {
     "duration": 0.005819,
     "end_time": "2025-11-11T12:35:16.417450",
     "exception": false,
     "start_time": "2025-11-11T12:35:16.411631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Qwen Fine-tuning for IRG Pipeline\n",
    "## Phase 2: Fine-tune Qwen 2.5 on Visual Reasoning Tasks\n",
    "\n",
    "This notebook fine-tunes Qwen using LoRA/QLoRA for memory efficiency.\n",
    "\n",
    "**GPU Required:** T4 x2 (32GB VRAM) recommended\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "**Required Inputs:**\n",
    "- Qwen model: Add from Kaggle datasets\n",
    "- Training data: From Notebook 1 (upload as dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722fe3b",
   "metadata": {
    "papermill": {
     "duration": 0.004288,
     "end_time": "2025-11-11T12:35:16.426583",
     "exception": false,
     "start_time": "2025-11-11T12:35:16.422295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361d1a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:35:16.436684Z",
     "iopub.status.busy": "2025-11-11T12:35:16.436011Z",
     "iopub.status.idle": "2025-11-11T12:36:34.426109Z",
     "shell.execute_reply": "2025-11-11T12:36:34.424960Z"
    },
    "papermill": {
     "duration": 77.996895,
     "end_time": "2025-11-11T12:36:34.427942",
     "exception": false,
     "start_time": "2025-11-11T12:35:16.431047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mâœ… Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.35.0 \\\n",
    "    peft>=0.7.0 \\\n",
    "    bitsandbytes>=0.41.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    datasets \\\n",
    "    tqdm \\\n",
    "    pandas \\\n",
    "    wandb\n",
    "\n",
    "print(\"âœ… Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b5274",
   "metadata": {
    "papermill": {
     "duration": 0.012024,
     "end_time": "2025-11-11T12:36:34.445359",
     "exception": false,
     "start_time": "2025-11-11T12:36:34.433335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Check GPU & Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b412295c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:36:34.457764Z",
     "iopub.status.busy": "2025-11-11T12:36:34.457448Z",
     "iopub.status.idle": "2025-11-11T12:36:38.135005Z",
     "shell.execute_reply": "2025-11-11T12:36:38.133943Z"
    },
    "papermill": {
     "duration": 3.686431,
     "end_time": "2025-11-11T12:36:38.136325",
     "exception": false,
     "start_time": "2025-11-11T12:36:34.449894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Information:\n",
      "============================================================\n",
      "âœ“ CUDA available: Tesla T4\n",
      "âœ“ GPU Count: 2\n",
      "  GPU 0: Tesla T4\n",
      "    Total Memory: 14.74 GB\n",
      "  GPU 1: Tesla T4\n",
      "    Total Memory: 14.74 GB\n",
      "\n",
      "Disk Space:\n",
      "  Total: 19 GB\n",
      "  Free: 19 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"GPU Information:\")\n",
    "print(\"=\"*60)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ“ GPU Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        mem_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"    Total Memory: {mem_total:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU available - this notebook requires GPU!\")\n",
    "    print(\"   Go to Settings â†’ Accelerator â†’ GPU T4 x2\")\n",
    "\n",
    "# Check disk space\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage(\"/kaggle/working\")\n",
    "print(f\"\\nDisk Space:\")\n",
    "print(f\"  Total: {total // (2**30)} GB\")\n",
    "print(f\"  Free: {free // (2**30)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599820eb",
   "metadata": {
    "papermill": {
     "duration": 0.00454,
     "end_time": "2025-11-11T12:36:38.146120",
     "exception": false,
     "start_time": "2025-11-11T12:36:38.141580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Verify Input Datasets\n",
    "\n",
    "**Before running:** Make sure you've added these datasets as inputs:\n",
    "1. Qwen model (search \"qwen2.5\" on Kaggle datasets)\n",
    "2. Your training data from Notebook 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "338cfb45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:36:38.156034Z",
     "iopub.status.busy": "2025-11-11T12:36:38.155422Z",
     "iopub.status.idle": "2025-11-11T12:36:38.168613Z",
     "shell.execute_reply": "2025-11-11T12:36:38.167929Z"
    },
    "papermill": {
     "duration": 0.019348,
     "end_time": "2025-11-11T12:36:38.169729",
     "exception": false,
     "start_time": "2025-11-11T12:36:38.150381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found Qwen model at: /kaggle/input/qwen2.5/transformers/3b-instruct/1\n",
      "âœ“ Found training data at: /kaggle/input/irg-1-dataset-generation/irg_training_data_improved\n",
      "  Files: ['complete_improved_dataset.json', 'train_improved.json', 'test_improved.json', 'train_improved.csv', 'dataset_stats.png', 'complete_improved_dataset.csv', 'test_improved.csv', 'val_improved.json', 'val_improved.csv']\n",
      "\n",
      "============================================================\n",
      "âœ… All inputs verified - ready to fine-tune!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check for Qwen model - adjust path based on your dataset\n",
    "# Common paths:\n",
    "QWEN_PATHS = [\n",
    "    \"/kaggle/input/qwen2.5/transformers/3b-instruct/1\"\n",
    "]\n",
    "\n",
    "QWEN_MODEL_PATH = None\n",
    "for path in QWEN_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        QWEN_MODEL_PATH = path\n",
    "        print(f\"âœ“ Found Qwen model at: {path}\")\n",
    "        break\n",
    "\n",
    "if QWEN_MODEL_PATH is None:\n",
    "    print(\"âš ï¸  Qwen model not found!\")\n",
    "    print(\"   Available inputs:\")\n",
    "    for item in os.listdir(\"/kaggle/input\"):\n",
    "        print(f\"     - {item}\")\n",
    "    print(\"\\n   Please add Qwen model as input dataset\")\n",
    "\n",
    "# Check for training data\n",
    "TRAINING_DATA_PATHS = [\n",
    "    \"/kaggle/input/irg-1-dataset-generation/irg_training_data_improved\",\n",
    "    \"/kaggle/input/irg_training_data\",\n",
    "]\n",
    "\n",
    "TRAINING_DATA_PATH = None\n",
    "for path in TRAINING_DATA_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        TRAINING_DATA_PATH = path\n",
    "        print(f\"âœ“ Found training data at: {path}\")\n",
    "        # List files\n",
    "        files = os.listdir(path)\n",
    "        print(f\"  Files: {files}\")\n",
    "        break\n",
    "\n",
    "if TRAINING_DATA_PATH is None:\n",
    "    print(\"âš ï¸  Training data not found!\")\n",
    "    print(\"   Please add your training dataset from Notebook 1 as input\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if QWEN_MODEL_PATH and TRAINING_DATA_PATH:\n",
    "    print(\"âœ… All inputs verified - ready to fine-tune!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Missing required inputs - add them before continuing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d97d0",
   "metadata": {
    "papermill": {
     "duration": 0.004552,
     "end_time": "2025-11-11T12:36:38.178891",
     "exception": false,
     "start_time": "2025-11-11T12:36:38.174339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Load Fine-tuning Script\n",
    "**Upload `qwen_finetune.py` to this notebook before running this cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4413c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:36:38.190247Z",
     "iopub.status.busy": "2025-11-11T12:36:38.190026Z",
     "iopub.status.idle": "2025-11-11T12:36:38.208830Z",
     "shell.execute_reply": "2025-11-11T12:36:38.208125Z"
    },
    "papermill": {
     "duration": 0.026293,
     "end_time": "2025-11-11T12:36:38.209931",
     "exception": false,
     "start_time": "2025-11-11T12:36:38.183638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen_finetune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen_finetune.py\n",
    "\n",
    "\"\"\"\n",
    "Qwen Fine-tuning System for Enhanced IRG Pipeline Performance\n",
    "Optimizes Qwen for better visual reasoning and image generation guidance\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "@dataclass\n",
    "class FineTuneConfig:\n",
    "    \"\"\"Configuration for Qwen fine-tuning\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_path: str = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct/1\"\n",
    "    output_dir: str = \"./qwen_irg_finetuned\"\n",
    "    \n",
    "    # LoRA configuration\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 32  # Rank\n",
    "    lora_alpha: int = 64  # Scaling parameter\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Optimization\n",
    "    use_8bit: bool = False\n",
    "    use_4bit: bool = True  # QLoRA\n",
    "    bnb_4bit_compute_dtype: str = \"float16\"\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    use_nested_quant: bool = True\n",
    "    \n",
    "    # Data settings\n",
    "    max_seq_length: int = 2048\n",
    "    train_split: float = 0.9\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 100\n",
    "    eval_steps: int = 50\n",
    "    save_total_limit: int = 3\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"qwen-irg-finetune\"\n",
    "\n",
    "# ==================== DATASET CREATION ====================\n",
    "\n",
    "class IRGReasoningDataset:\n",
    "    \"\"\"\n",
    "    Create specialized dataset for training Qwen on visual reasoning tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_visual_reasoning_examples() -> List[Dict[str, str]]:\n",
    "        \"\"\"Create high-quality visual reasoning examples\"\"\"\n",
    "        \n",
    "        examples = []\n",
    "        \n",
    "        # Template categories for comprehensive training\n",
    "        templates = {\n",
    "            \"composition\": [\n",
    "                {\n",
    "                    \"prompt\": \"A majestic lion resting under an acacia tree at sunset\",\n",
    "                    \"reasoning\": \"Compose the scene with the lion as the focal point in the lower third, positioned slightly off-center using the rule of thirds. The acacia tree should frame the composition from the left, creating depth. Use golden hour lighting with warm oranges and deep shadows. The sunset should create rim lighting on the lion's mane. Include savanna grass in the foreground with bokeh effect. Add atmospheric haze for depth. Use a low camera angle to emphasize the lion's majesty.\",\n",
    "                    \"refinement\": \"Enhance the rim lighting on the lion's mane to create more dramatic contrast. Add more detail to the lion's eyes - they should reflect the sunset light. Adjust the acacia tree's silhouette to be more distinctive. Increase the warmth in the golden hour tones. Add dust particles in the air catching the light. Refine the grass texture in the foreground for better realism.\"\n",
    "                },\n",
    "                {\n",
    "                    \"prompt\": \"A futuristic cityscape with flying vehicles and neon lights\",\n",
    "                    \"reasoning\": \"Create a vertical composition emphasizing the height of skyscrapers. Use cyberpunk aesthetic with dominant cyan and magenta neon colors. Position multiple flying vehicles at different depths for scale. Apply atmospheric perspective with fog in the distance. Include reflective surfaces on buildings to multiply the neon lights. Use a slightly tilted camera angle for dynamism. Add rain for enhanced reflections and mood.\",\n",
    "                    \"refinement\": \"Intensify the neon glow effects with proper bloom. Add more variety to the flying vehicle designs. Enhance the rain streaks and their interaction with lights. Increase detail in building textures - add more windows, balconies, and architectural elements. Adjust the fog density for better depth separation. Add subtle lens flares from bright light sources.\"\n",
    "                }\n",
    "            ],\n",
    "            \"lighting\": [\n",
    "                {\n",
    "                    \"prompt\": \"A still life with fruits in dramatic chiaroscuro lighting\",\n",
    "                    \"reasoning\": \"Set up strong directional lighting from the top-left at 45 degrees. Create deep shadows for dramatic contrast. Use a single key light source to emulate Caravaggio's technique. Arrange fruits (apples, grapes, pears) with varying textures and translucency. Position them to create interesting shadow patterns. Use a dark background to emphasize the light-dark contrast. Add subtle rim lighting to separate subjects from background.\",\n",
    "                    \"refinement\": \"Increase the contrast between light and shadow areas. Add subsurface scattering to grapes for translucency. Enhance the texture details on fruit surfaces - show imperfections and natural patterns. Adjust the shadow edges - softer for distant objects, sharper for close ones. Add subtle reflected light in shadow areas from nearby fruits.\"\n",
    "                }\n",
    "            ],\n",
    "            \"detail_enhancement\": [\n",
    "                {\n",
    "                    \"prompt\": \"An elderly person's portrait showing wisdom and experience\",\n",
    "                    \"reasoning\": \"Focus on capturing fine details in facial features. Use soft, diffused lighting from a window. Emphasize wrinkles, age spots, and texture in skin. Capture the depth in eyes with catchlights. Use shallow depth of field with eyes in sharp focus. Include subtle details like individual hair strands, fabric texture in clothing. Apply Rembrandt lighting for character.\",\n",
    "                    \"refinement\": \"Enhance the micro-details in skin texture - pores, fine lines, age spots. Add more depth to the eyes with subtle color variations in the iris. Refine individual hair strands and eyebrows. Increase the catchlight clarity. Add subtle veins visible under thin skin. Enhance fabric texture with visible weave patterns.\"\n",
    "                }\n",
    "            ],\n",
    "            \"atmosphere\": [\n",
    "                {\n",
    "                    \"prompt\": \"A misty forest path at dawn\",\n",
    "                    \"reasoning\": \"Create layered depth with multiple fog density levels. Use cool blue-green color palette for dawn atmosphere. Position trees to create a natural leading line along the path. Apply volumetric lighting with sun rays filtering through trees. Add dew drops on leaves and spider webs. Use atmospheric perspective with distant trees fading into mist. Include ground fog rolling across the path.\",\n",
    "                    \"refinement\": \"Increase the variation in fog density between tree layers. Enhance the god rays with more defined light shafts. Add more dew drops catching the light. Refine the tree bark textures with moss and lichen details. Adjust the color temperature gradient from cool shadows to warm highlights. Add subtle movement blur to fog for dynamism.\"\n",
    "                }\n",
    "            ],\n",
    "            \"style_specific\": [\n",
    "                {\n",
    "                    \"prompt\": \"A samurai warrior in traditional armor\",\n",
    "                    \"reasoning\": \"Compose with strong diagonal lines from the katana. Use dramatic lighting to highlight armor details and create depth. Apply rich, saturated colors - deep reds, blacks, and gold accents. Focus on the intricate patterns in the armor plates. Create a sense of power through low camera angle. Add atmospheric elements like subtle smoke or mist. Ensure historically accurate details in armor design.\",\n",
    "                    \"refinement\": \"Enhance the metallic reflections on armor plates. Add more intricate details to the armor lacing and patterns. Refine the katana blade with proper hamon line and reflections. Increase the texture detail in fabric elements. Add subtle battle wear and patina to armor. Enhance the facial expression for more intensity.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Generate examples from templates\n",
    "        for category, items in templates.items():\n",
    "            for item in items:\n",
    "                # Initial reasoning example\n",
    "                examples.append({\n",
    "                    \"instruction\": f\"Analyze this prompt and provide detailed visual reasoning for image generation: \\\"{item['prompt']}\\\"\",\n",
    "                    \"input\": item['prompt'],\n",
    "                    \"output\": item['reasoning'],\n",
    "                    \"type\": \"initial_reasoning\"\n",
    "                })\n",
    "                \n",
    "                # Refinement reasoning example\n",
    "                examples.append({\n",
    "                    \"instruction\": f\"Based on the current image state, provide specific refinement instructions for improvement. Original prompt: \\\"{item['prompt']}\\\"\",\n",
    "                    \"input\": f\"Prompt: {item['prompt']}\\nCurrent state: Image has been generated with basic composition.\",\n",
    "                    \"output\": item['refinement'],\n",
    "                    \"type\": \"refinement_reasoning\"\n",
    "                })\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_quality_improvement_examples() -> List[Dict[str, str]]:\n",
    "        \"\"\"Create examples for quality improvement reasoning\"\"\"\n",
    "        \n",
    "        quality_examples = [\n",
    "            {\n",
    "                \"instruction\": \"Identify quality issues and provide improvement suggestions\",\n",
    "                \"input\": \"The image appears blurry and lacks sharp details, especially in the focal area.\",\n",
    "                \"output\": \"To improve sharpness: 1) Increase the image guidance scale to 8.5 for better prompt adherence, 2) Add 'sharp focus, highly detailed, 4k, crisp details' to the positive prompt, 3) Reduce the denoising strength to 0.6 to preserve more detail, 4) Focus on enhancing edge definition in the main subject, 5) Add subtle sharpening in post-processing focusing on mid-frequency details.\",\n",
    "                \"type\": \"quality_fix\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Analyze color balance issues and suggest corrections\",\n",
    "                \"input\": \"The image has oversaturated colors and poor color harmony.\",\n",
    "                \"output\": \"Color correction approach: 1) Reduce global saturation by 15-20%, 2) Adjust individual color channels - particularly reduce reds and magentas, 3) Improve color harmony by shifting towards analogous color scheme, 4) Add 'balanced colors, natural saturation, harmonious palette' to the prompt, 5) Apply subtle color grading to unify the overall tone, 6) Ensure shadows and highlights have complementary color temperatures.\",\n",
    "                \"type\": \"quality_fix\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Address composition problems in the generated image\",\n",
    "                \"input\": \"The composition feels unbalanced with too much empty space on the right side.\",\n",
    "                \"output\": \"Composition improvements: 1) Apply rule of thirds - reposition main subject to left third vertical line, 2) Add secondary elements on the right to balance visual weight, 3) Create depth by adding foreground elements, 4) Use leading lines to guide the eye through the composition, 5) Adjust crop to eliminate excessive negative space, 6) Include 'balanced composition, rule of thirds, visual harmony' in the prompt.\",\n",
    "                \"type\": \"quality_fix\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return quality_examples\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_style_transfer_examples() -> List[Dict[str, str]]:\n",
    "        \"\"\"Create examples for style-specific reasoning\"\"\"\n",
    "        \n",
    "        style_examples = [\n",
    "            {\n",
    "                \"instruction\": \"Apply photorealistic style to this concept\",\n",
    "                \"input\": \"A dragon perched on a mountain peak\",\n",
    "                \"output\": \"For photorealistic rendering: 1) Focus on anatomically plausible dragon design based on reptilian references, 2) Apply realistic lighting with proper shadow casting and ambient occlusion, 3) Include atmospheric effects - fog, haze, and distance blur, 4) Add realistic textures - scales with iridescent properties, weathered rock surfaces, 5) Use 'photorealistic, 8k photography, ultra detailed, shot on Hasselblad' in prompt, 6) Ensure proper scale references with trees or birds, 7) Apply realistic color grading similar to nature photography.\",\n",
    "                \"type\": \"style_reasoning\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Convert to oil painting style\",\n",
    "                \"input\": \"A busy market street scene\",\n",
    "                \"output\": \"Oil painting stylization: 1) Apply visible brushstrokes with impasto technique for texture, 2) Use rich, saturated oil paint colors with subtle mixing, 3) Create depth through overlapping brush strokes and color temperature shifts, 4) Add 'oil painting, thick brushstrokes, impasto, traditional art, canvas texture' to prompt, 5) Emphasize light and shadow with bold strokes, 6) Slightly reduce fine details in favor of impressionistic rendering, 7) Include paint mixing effects at color boundaries.\",\n",
    "                \"type\": \"style_reasoning\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return style_examples\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_full_dataset() -> List[Dict[str, str]]:\n",
    "        \"\"\"Combine all examples into a complete dataset\"\"\"\n",
    "        \n",
    "        all_examples = []\n",
    "        all_examples.extend(IRGReasoningDataset.create_visual_reasoning_examples())\n",
    "        all_examples.extend(IRGReasoningDataset.create_quality_improvement_examples())\n",
    "        all_examples.extend(IRGReasoningDataset.create_style_transfer_examples())\n",
    "        \n",
    "        # Add prompt engineering examples\n",
    "        prompt_engineering = [\n",
    "            {\n",
    "                \"instruction\": \"Enhance this basic prompt with technical photography terms\",\n",
    "                \"input\": \"A cat sitting on a windowsill\",\n",
    "                \"output\": \"Enhanced prompt: 'A tabby cat gracefully perched on a weathered wooden windowsill, soft natural lighting, shallow depth of field with bokeh background, shot at golden hour, rule of thirds composition, Kodak Portra 400 film aesthetic, 85mm lens, f/1.8 aperture, subtle rim lighting highlighting fur texture, warm color grading, photorealistic detail'\",\n",
    "                \"type\": \"prompt_enhancement\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Add artistic style descriptors to this prompt\",\n",
    "                \"input\": \"A mountain landscape\",\n",
    "                \"output\": \"Artistic prompt: 'Majestic mountain landscape in the style of Albert Bierstadt, dramatic romantic lighting, sublime natural beauty, oil on canvas, thick impasto technique, Hudson River School aesthetic, golden hour illumination, atmospheric perspective with misty valleys, rich earth tones contrasting with snow-capped peaks, masterpiece quality, museum-worthy composition'\",\n",
    "                \"type\": \"prompt_enhancement\"\n",
    "            }\n",
    "        ]\n",
    "        all_examples.extend(prompt_engineering)\n",
    "        \n",
    "        return all_examples\n",
    "\n",
    "# ==================== CUSTOM DATASET CLASS ====================\n",
    "\n",
    "class QwenIRGDataset(Dataset):\n",
    "    \"\"\"PyTorch dataset for Qwen fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[Dict[str, str]],\n",
    "        tokenizer,\n",
    "        max_length: int = 2048,\n",
    "        is_training: bool = True\n",
    "    ):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Format as conversation\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert visual reasoning assistant specialized in providing detailed guidance for high-quality image generation.\"},\n",
    "            {\"role\": \"user\", \"content\": example['instruction']},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Set up labels for training\n",
    "        labels = encodings[\"input_ids\"].clone()\n",
    "        \n",
    "        # Find where assistant response starts and mask everything before\n",
    "        response_start = text.find(example['output'])\n",
    "        if response_start != -1:\n",
    "            # Mask tokens before the response\n",
    "            response_token_start = len(self.tokenizer.encode(text[:response_start]))\n",
    "            labels[0, :response_token_start] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels.squeeze()\n",
    "        }\n",
    "\n",
    "# ==================== TRAINING UTILITIES ====================\n",
    "\n",
    "class QwenFineTuner:\n",
    "    \"\"\"Main class for fine-tuning Qwen\"\"\"\n",
    "\n",
    "    def __init__(self, config: FineTuneConfig):\n",
    "        self.config = config\n",
    "        # Note: Don't use Accelerator when using Trainer with quantized models\n",
    "        # Trainer handles acceleration internally\n",
    "\n",
    "        # Initialize wandb if enabled\n",
    "        if config.use_wandb:\n",
    "            wandb.init(project=config.wandb_project, config=vars(config))\n",
    "    \n",
    "    def prepare_model_and_tokenizer(self):\n",
    "        \"\"\"Load and prepare model for fine-tuning\"\"\"\n",
    "        \n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.model_path,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Quantization config for QLoRA\n",
    "        bnb_config = None\n",
    "        if self.config.use_4bit:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=self.config.bnb_4bit_quant_type,\n",
    "                bnb_4bit_use_double_quant=self.config.use_nested_quant\n",
    "            )\n",
    "        elif self.config.use_8bit:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "        # Determine device map - for quantized models, use current device\n",
    "        if self.config.use_4bit or self.config.use_8bit:\n",
    "            device_map = {\"\": 0}  # Load everything on GPU 0\n",
    "        else:\n",
    "            device_map = \"auto\"\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_path,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16 if not (self.config.use_4bit or self.config.use_8bit) else None,\n",
    "            trust_remote_code=True,\n",
    "            device_map=device_map\n",
    "        )\n",
    "        \n",
    "        # Prepare model for training\n",
    "        if self.config.use_4bit or self.config.use_8bit:\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Apply LoRA if enabled\n",
    "        if self.config.use_lora:\n",
    "            print(\"Applying LoRA configuration...\")\n",
    "            lora_config = LoraConfig(\n",
    "                r=self.config.lora_r,\n",
    "                lora_alpha=self.config.lora_alpha,\n",
    "                target_modules=self.config.lora_target_modules,\n",
    "                lora_dropout=self.config.lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM\n",
    "            )\n",
    "            model = get_peft_model(model, lora_config)\n",
    "            model.print_trainable_parameters()\n",
    "        \n",
    "        return model, tokenizer\n",
    "    \n",
    "    def prepare_datasets(self, tokenizer, external_train_data=None, external_val_data=None):\n",
    "        \"\"\"Prepare training and validation datasets\"\"\"\n",
    "\n",
    "        print(\"Preparing datasets...\")\n",
    "\n",
    "        # Use external data if provided, otherwise use built-in\n",
    "        if external_train_data is not None and external_val_data is not None:\n",
    "            print(\"Using external dataset (from Phase 1)\")\n",
    "            train_examples = external_train_data\n",
    "            val_examples = external_val_data\n",
    "        else:\n",
    "            print(\"Using built-in dataset (small - not recommended)\")\n",
    "            # Create examples\n",
    "            examples = IRGReasoningDataset.create_full_dataset()\n",
    "\n",
    "            # Augment with more examples\n",
    "            examples = self.augment_dataset(examples)\n",
    "\n",
    "            # Split into train/val\n",
    "            split_idx = int(len(examples) * self.config.train_split)\n",
    "            train_examples = examples[:split_idx]\n",
    "            val_examples = examples[split_idx:]\n",
    "\n",
    "        print(f\"Training examples: {len(train_examples)}\")\n",
    "        print(f\"Validation examples: {len(val_examples)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = QwenIRGDataset(\n",
    "            train_examples,\n",
    "            tokenizer,\n",
    "            self.config.max_seq_length,\n",
    "            is_training=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = QwenIRGDataset(\n",
    "            val_examples,\n",
    "            tokenizer,\n",
    "            self.config.max_seq_length,\n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def augment_dataset(self, examples: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Augment dataset with variations\"\"\"\n",
    "        \n",
    "        augmented = examples.copy()\n",
    "        \n",
    "        # Add variations for each example\n",
    "        for example in examples:\n",
    "            if example['type'] == 'initial_reasoning':\n",
    "                # Create variation with different focus\n",
    "                variation = example.copy()\n",
    "                variation['instruction'] = variation['instruction'].replace(\n",
    "                    \"provide detailed visual reasoning\",\n",
    "                    \"focus on composition and lighting\"\n",
    "                )\n",
    "                augmented.append(variation)\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def create_training_args(self):\n",
    "        \"\"\"Create training arguments\"\"\"\n",
    "        \n",
    "        return TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_total_limit=self.config.save_total_limit,\n",
    "            eval_strategy=\"steps\" if self.config.eval_steps < 1000000 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=True if self.config.eval_steps < 1000000 else False,\n",
    "            metric_for_best_model=\"loss\" if self.config.eval_steps < 1000000 else None,\n",
    "            greater_is_better=False,\n",
    "            push_to_hub=False,\n",
    "            report_to=[\"wandb\"] if self.config.use_wandb else [\"none\"],\n",
    "            bf16=True,  # BFloat16 is more stable than FP16\n",
    "            fp16=False,  # Disable FP16 to avoid CUBLAS errors\n",
    "            gradient_checkpointing=True,\n",
    "            max_grad_norm=self.config.max_grad_norm,\n",
    "            optim=\"paged_adamw_8bit\" if self.config.use_4bit else \"adamw_torch\",\n",
    "            seed=self.config.seed,\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False,  # Reduce memory pressure\n",
    "            ddp_find_unused_parameters=False,  # Stability for distributed training\n",
    "        )\n",
    "    \n",
    "    def train(self, external_train_data=None, external_val_data=None):\n",
    "        \"\"\"Main training function\"\"\"\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        print(\"Starting Qwen Fine-tuning for IRG Pipeline\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Prepare model and tokenizer\n",
    "        model, tokenizer = self.prepare_model_and_tokenizer()\n",
    "\n",
    "        # Prepare datasets\n",
    "        train_dataset, val_dataset = self.prepare_datasets(\n",
    "            tokenizer,\n",
    "            external_train_data,\n",
    "            external_val_data\n",
    "        )\n",
    "        \n",
    "        # Create training arguments\n",
    "        training_args = self.create_training_args()\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        # Create memory cleanup callback\n",
    "        from transformers import TrainerCallback\n",
    "        import gc\n",
    "\n",
    "        class MemoryCleanupCallback(TrainerCallback):\n",
    "            \"\"\"Aggressively clean memory after evaluation to prevent CUBLAS errors\"\"\"\n",
    "            def on_evaluate(self, args, state, control, **kwargs):\n",
    "                \"\"\"Clean up after evaluation\"\"\"\n",
    "                print(\"\\nðŸ§¹ Cleaning GPU memory after evaluation...\")\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                if torch.cuda.is_available():\n",
    "                    for i in range(torch.cuda.device_count()):\n",
    "                        torch.cuda.reset_peak_memory_stats(i)\n",
    "                print(\"âœ… Memory cleaned\\n\")\n",
    "                return control\n",
    "\n",
    "        # Create trainer with memory cleanup callback\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            callbacks=[MemoryCleanupCallback()],\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        print(\"\\nðŸš€ Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save final model\n",
    "        print(\"\\nðŸ’¾ Saving fine-tuned model...\")\n",
    "        trainer.save_model(self.config.output_dir)\n",
    "        tokenizer.save_pretrained(self.config.output_dir)\n",
    "        \n",
    "        # Save LoRA weights separately if used\n",
    "        if self.config.use_lora:\n",
    "            model.save_pretrained(f\"{self.config.output_dir}/lora_weights\")\n",
    "        \n",
    "        print(f\"\\nâœ… Fine-tuning complete! Model saved to {self.config.output_dir}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "\n",
    "# ==================== INFERENCE WITH FINE-TUNED MODEL ====================\n",
    "\n",
    "class OptimizedQwenInference:\n",
    "    \"\"\"Use the fine-tuned Qwen for improved IRG pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, use_lora: bool = True):\n",
    "        self.model_path = model_path\n",
    "        self.use_lora = use_lora\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load fine-tuned model\"\"\"\n",
    "        \n",
    "        print(f\"Loading fine-tuned model from {self.model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_path,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.use_lora:\n",
    "            # Load base model + LoRA weights\n",
    "            from peft import PeftModel\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                f\"{self.model_path}/lora_weights\"\n",
    "            )\n",
    "            self.model = self.model.merge_and_unload()  # Merge LoRA weights\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_reasoning(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        reasoning_type: str = \"initial\",\n",
    "        max_length: int = 500,\n",
    "        temperature: float = 0.7\n",
    "    ) -> str:\n",
    "        \"\"\"Generate improved reasoning with fine-tuned model\"\"\"\n",
    "        \n",
    "        # Create appropriate instruction based on type\n",
    "        if reasoning_type == \"initial\":\n",
    "            instruction = f\"Analyze this prompt and provide detailed visual reasoning for image generation: \\\"{prompt}\\\"\"\n",
    "        elif reasoning_type == \"refinement\":\n",
    "            instruction = f\"Based on the current image state, provide specific refinement instructions for improvement. Original prompt: \\\"{prompt}\\\"\"\n",
    "        else:\n",
    "            instruction = prompt\n",
    "        \n",
    "        # Format as conversation\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert visual reasoning assistant specialized in providing detailed guidance for high-quality image generation.\"},\n",
    "            {\"role\": \"user\", \"content\": instruction}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract assistant response\n",
    "        if \"assistant\" in response:\n",
    "            response = response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "# ==================== EVALUATION ====================\n",
    "\n",
    "class FineTuneEvaluator:\n",
    "    \"\"\"Evaluate the fine-tuned model's performance\"\"\"\n",
    "    \n",
    "    def __init__(self, original_model_path: str, finetuned_model_path: str):\n",
    "        self.original_path = original_model_path\n",
    "        self.finetuned_path = finetuned_model_path\n",
    "        \n",
    "    def compare_outputs(self, test_prompts: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Compare outputs between original and fine-tuned models\"\"\"\n",
    "        \n",
    "        # Load both models\n",
    "        original_inference = OptimizedQwenInference(\n",
    "            self.original_path,\n",
    "            use_lora=False\n",
    "        )\n",
    "        finetuned_inference = OptimizedQwenInference(\n",
    "            self.finetuned_path,\n",
    "            use_lora=True\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for prompt in test_prompts:\n",
    "            # Generate with both models\n",
    "            original_output = original_inference.generate_reasoning(prompt)\n",
    "            finetuned_output = finetuned_inference.generate_reasoning(prompt)\n",
    "            \n",
    "            # Simple quality metrics\n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"original_length\": len(original_output),\n",
    "                \"finetuned_length\": len(finetuned_output),\n",
    "                \"original_detail_keywords\": self._count_detail_keywords(original_output),\n",
    "                \"finetuned_detail_keywords\": self._count_detail_keywords(finetuned_output),\n",
    "                \"original_output\": original_output[:200] + \"...\",\n",
    "                \"finetuned_output\": finetuned_output[:200] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _count_detail_keywords(self, text: str) -> int:\n",
    "        \"\"\"Count visual detail keywords in output\"\"\"\n",
    "        keywords = [\n",
    "            \"lighting\", \"shadow\", \"composition\", \"texture\", \"color\",\n",
    "            \"detail\", \"contrast\", \"depth\", \"focus\", \"atmosphere\",\n",
    "            \"reflection\", \"highlight\", \"tone\", \"saturation\", \"sharpness\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return sum(1 for keyword in keywords if keyword in text_lower)\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Configuration\n",
    "    config = FineTuneConfig(\n",
    "        model_path=\"/kaggle/input/qwen2.5/transformers/0.5b-instruct/1\",\n",
    "        output_dir=\"./qwen_irg_finetuned\",\n",
    "        use_lora=True,\n",
    "        lora_r=32,\n",
    "        lora_alpha=64,\n",
    "        num_epochs=3,\n",
    "        batch_size=4,\n",
    "        learning_rate=2e-4,\n",
    "        use_4bit=True,  # QLoRA for memory efficiency\n",
    "        use_wandb=False  # Set True if you have wandb configured\n",
    "    )\n",
    "    \n",
    "    # Initialize fine-tuner\n",
    "    fine_tuner = QwenFineTuner(config)\n",
    "    \n",
    "    # Start fine-tuning\n",
    "    model, tokenizer = fine_tuner.train()\n",
    "    \n",
    "    # Test the fine-tuned model\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Testing Fine-tuned Model\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    inference = OptimizedQwenInference(\n",
    "        config.output_dir,\n",
    "        use_lora=config.use_lora\n",
    "    )\n",
    "    \n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"A serene lake at sunset with mountains in the background\",\n",
    "        \"A cyberpunk street scene with neon lights\",\n",
    "        \"A medieval castle on a hilltop during a storm\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nðŸ“ Prompt: {prompt}\")\n",
    "        reasoning = inference.generate_reasoning(prompt, reasoning_type=\"initial\")\n",
    "        print(f\"ðŸ¤– Reasoning: {reasoning[:300]}...\")\n",
    "    \n",
    "    print(\"\\nâœ… Fine-tuning and testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45d85a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:36:38.220262Z",
     "iopub.status.busy": "2025-11-11T12:36:38.219708Z",
     "iopub.status.idle": "2025-11-11T12:37:16.482034Z",
     "shell.execute_reply": "2025-11-11T12:37:16.481046Z"
    },
    "papermill": {
     "duration": 38.269217,
     "end_time": "2025-11-11T12:37:16.483585",
     "exception": false,
     "start_time": "2025-11-11T12:36:38.214368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 12:36:51.562755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762864611.763971      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762864611.826257      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fine-tuning modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working')\n",
    "\n",
    "# Import fine-tuning components\n",
    "from qwen_finetune import (\n",
    "    QwenFineTuner,\n",
    "    FineTuneConfig,\n",
    "    IRGReasoningDataset\n",
    ")\n",
    "\n",
    "print(\"âœ“ Fine-tuning modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0f3d4",
   "metadata": {
    "papermill": {
     "duration": 0.004506,
     "end_time": "2025-11-11T12:37:16.493330",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.488824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Configure Fine-tuning (Optimized for T4 x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a507d4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:37:16.503975Z",
     "iopub.status.busy": "2025-11-11T12:37:16.503633Z",
     "iopub.status.idle": "2025-11-11T12:37:16.516013Z",
     "shell.execute_reply": "2025-11-11T12:37:16.515174Z"
    },
    "papermill": {
     "duration": 0.019255,
     "end_time": "2025-11-11T12:37:16.517220",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.497965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Configuration for Qwen 2.5-7B:\n",
      "============================================================\n",
      "Model: /kaggle/input/qwen2.5/transformers/3b-instruct/1\n",
      "Output: /kaggle/working/qwen_irg_finetuned\n",
      "\n",
      "Training:\n",
      "  Epochs: 3\n",
      "  Batch size: 1 (REDUCED for 7B)\n",
      "  Gradient accumulation: 8\n",
      "  Effective batch size: 8\n",
      "  Learning rate: 0.0002\n",
      "\n",
      "Memory Optimization (7B-specific):\n",
      "  4-bit quantization: True\n",
      "  LoRA rank: 8 (REDUCED for memory)\n",
      "  Max sequence length: 1024 (REDUCED from 2048)\n",
      "\n",
      "Expected Memory Usage:\n",
      "  Model (4-bit): ~3.5-4GB\n",
      "  LoRA adapters (r=8): ~0.5GB\n",
      "  Activations + gradients: ~8-10GB\n",
      "  Total estimated: ~12-15GB per GPU\n",
      "\n",
      "Checkpointing:\n",
      "  Save every 100 steps\n",
      "  Evaluate every 10000000 steps\n",
      "============================================================\n",
      "\n",
      "âš ï¸  NOTE: If still OOM, further reduce max_seq_length to 256\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning configuration optimized for Qwen 2.5-7B on T4 x2 (32GB VRAM)\n",
    "# IMPORTANT: This config is specifically for 7B model (much more aggressive than 0.5B)\n",
    "config = FineTuneConfig(\n",
    "    # Model paths\n",
    "    model_path=QWEN_MODEL_PATH,\n",
    "    output_dir=\"/kaggle/working/qwen_irg_finetuned\",\n",
    "    \n",
    "    # LoRA settings (REDUCED for 7B model)\n",
    "    use_lora=True,\n",
    "    lora_r=8,               # REDUCED from 32 â†’ 8 (less trainable params)\n",
    "    lora_alpha=16,          # REDUCED from 64 â†’ 16 (2x rank)\n",
    "    lora_dropout=0.1,\n",
    "    \n",
    "    # Training parameters (OPTIMIZED for 7B)\n",
    "    num_epochs=3,           # 3 epochs is usually sufficient\n",
    "    batch_size=1,           # REDUCED from 2 â†’ 1 (critical for 7B!)\n",
    "    gradient_accumulation_steps=8,  # INCREASED from 4 â†’ 8 (effective batch = 1*8 = 8)\n",
    "    learning_rate=2e-4,     # Standard for LoRA\n",
    "    warmup_ratio=0.1,       # 10% warmup\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Memory optimization (CRITICAL for 7B on T4)\n",
    "    use_4bit=True,          # QLoRA - 4-bit quantization (ESSENTIAL!)\n",
    "    use_8bit=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    use_nested_quant=True,  # Double quantization for extra savings\n",
    "    \n",
    "    # Data settings (REDUCED for 7B)\n",
    "    max_seq_length=1024,     # REDUCED from 2048 â†’ 512 (huge memory savings!)\n",
    "    train_split=0.9,        # 90% train, 10% validation\n",
    "    seed=42,\n",
    "    \n",
    "    # Checkpointing (IMPORTANT for Kaggle timeout protection)\n",
    "    logging_steps=10,       # Log every 10 steps\n",
    "    save_steps=100,         # Save checkpoint every 100 steps\n",
    "    eval_steps=10000000,          # Evaluate every 50 steps\n",
    "    save_total_limit=3,     # Keep only last 3 checkpoints\n",
    "    \n",
    "    # Logging\n",
    "    use_wandb=False,        # Set True if you have wandb configured\n",
    "    wandb_project=\"qwen-irg-finetune-7b\"\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning Configuration for Qwen 2.5-7B:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {config.model_path}\")\n",
    "print(f\"Output: {config.output_dir}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Batch size: {config.batch_size} (REDUCED for 7B)\")\n",
    "print(f\"  Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"\\nMemory Optimization (7B-specific):\")\n",
    "print(f\"  4-bit quantization: {config.use_4bit}\")\n",
    "print(f\"  LoRA rank: {config.lora_r} (REDUCED for memory)\")\n",
    "print(f\"  Max sequence length: {config.max_seq_length} (REDUCED from 2048)\")\n",
    "print(f\"\\nExpected Memory Usage:\")\n",
    "print(f\"  Model (4-bit): ~3.5-4GB\")\n",
    "print(f\"  LoRA adapters (r=8): ~0.5GB\")\n",
    "print(f\"  Activations + gradients: ~8-10GB\")\n",
    "print(f\"  Total estimated: ~12-15GB per GPU\")\n",
    "print(f\"\\nCheckpointing:\")\n",
    "print(f\"  Save every {config.save_steps} steps\")\n",
    "print(f\"  Evaluate every {config.eval_steps} steps\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâš ï¸  NOTE: If still OOM, further reduce max_seq_length to 256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454510a",
   "metadata": {
    "papermill": {
     "duration": 0.004477,
     "end_time": "2025-11-11T12:37:16.526720",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.522243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. Load or Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d978837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:37:16.537641Z",
     "iopub.status.busy": "2025-11-11T12:37:16.537069Z",
     "iopub.status.idle": "2025-11-11T12:37:16.683562Z",
     "shell.execute_reply": "2025-11-11T12:37:16.682658Z"
    },
    "papermill": {
     "duration": 0.153615,
     "end_time": "2025-11-11T12:37:16.685167",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.531552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded external dataset:\n",
      "  Training examples: 3600\n",
      "  Validation examples: 400\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Option 1: Use generated dataset from Notebook 1 (RECOMMENDED)\n",
    "if TRAINING_DATA_PATH:\n",
    "    train_file = os.path.join(TRAINING_DATA_PATH, 'complete_improved_dataset.json')\n",
    "    val_file = os.path.join(TRAINING_DATA_PATH, 'val.json')\n",
    "    \n",
    "    if os.path.exists(train_file):\n",
    "        with open(train_file, 'r') as f:\n",
    "            train_examples = json.load(f)\n",
    "        \n",
    "        if os.path.exists(val_file):\n",
    "            with open(val_file, 'r') as f:\n",
    "                val_examples = json.load(f)\n",
    "        else:\n",
    "            # Create validation split if not present\n",
    "            split_idx = int(len(train_examples) * 0.9)\n",
    "            val_examples = train_examples[split_idx:]\n",
    "            train_examples = train_examples[:split_idx]\n",
    "        \n",
    "        print(f\"âœ“ Loaded external dataset:\")\n",
    "        print(f\"  Training examples: {len(train_examples)}\")\n",
    "        print(f\"  Validation examples: {len(val_examples)}\")\n",
    "        \n",
    "        USE_EXTERNAL_DATA = True\n",
    "    else:\n",
    "        print(\"âš ï¸  train.json not found in training data path\")\n",
    "        USE_EXTERNAL_DATA = False\n",
    "else:\n",
    "    USE_EXTERNAL_DATA = False\n",
    "\n",
    "# Option 2: Use built-in dataset (fallback)\n",
    "if not USE_EXTERNAL_DATA:\n",
    "    print(\"Using built-in dataset from qwen_finetune.py\")\n",
    "    print(\"âš ï¸  This is a smaller dataset - external data recommended for better results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e8716",
   "metadata": {
    "papermill": {
     "duration": 0.006177,
     "end_time": "2025-11-11T12:37:16.701921",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.695744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. Initialize Fine-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbf637e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:37:16.713332Z",
     "iopub.status.busy": "2025-11-11T12:37:16.712791Z",
     "iopub.status.idle": "2025-11-11T12:37:16.717979Z",
     "shell.execute_reply": "2025-11-11T12:37:16.717025Z"
    },
    "papermill": {
     "duration": 0.012307,
     "end_time": "2025-11-11T12:37:16.719270",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.706963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fine-tuner initialized\n",
      "  This loaded the Accelerator for distributed training\n"
     ]
    }
   ],
   "source": [
    "# Initialize the fine-tuner\n",
    "fine_tuner = QwenFineTuner(config)\n",
    "\n",
    "print(\"âœ“ Fine-tuner initialized\")\n",
    "print(\"  This loaded the Accelerator for distributed training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11930d7a",
   "metadata": {
    "papermill": {
     "duration": 0.00497,
     "end_time": "2025-11-11T12:37:16.729446",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.724476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8. Start Fine-tuning\n",
    "\n",
    "**This will take 3-4 hours on T4 x2**\n",
    "\n",
    "**Checkpoints** will be saved every 100 steps to `/kaggle/working/qwen_irg_finetuned/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59fcf5a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T12:37:16.740846Z",
     "iopub.status.busy": "2025-11-11T12:37:16.740618Z",
     "iopub.status.idle": "2025-11-11T18:22:32.644085Z",
     "shell.execute_reply": "2025-11-11T18:22:32.643191Z"
    },
    "papermill": {
     "duration": 20715.910908,
     "end_time": "2025-11-11T18:22:32.645410",
     "exception": false,
     "start_time": "2025-11-11T12:37:16.734502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Fine-tuning\n",
      "============================================================\n",
      "Start time: 2025-11-11 12:37:16\n",
      "\n",
      "This will take approximately 3-4 hours...\n",
      "You can monitor progress below\n",
      "\n",
      "âš ï¸ IMPORTANT: Using 3600 training examples from external dataset\n",
      "\n",
      "================================================================================\n",
      "Starting Qwen Fine-tuning for IRG Pipeline\n",
      "================================================================================\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc97044f56de499db0f560d0bd124b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/qwen_finetune.py:517: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,966,784 || all params: 3,100,905,472 || trainable%: 0.4827\n",
      "Preparing datasets...\n",
      "Using external dataset (from Phase 1)\n",
      "Training examples: 3600\n",
      "Validation examples: 400\n",
      "\n",
      "ðŸš€ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 5:43:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Saving fine-tuned model...\n",
      "\n",
      "âœ… Fine-tuning complete! Model saved to /kaggle/working/qwen_irg_finetuned\n",
      "\n",
      "============================================================\n",
      "âœ… Fine-tuning Complete!\n",
      "============================================================\n",
      "End time: 2025-11-11 18:22:32\n",
      "Total time: 5.75 hours\n",
      "Model saved to: /kaggle/working/qwen_irg_finetuned\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Fine-tuning\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nThis will take approximately 3-4 hours...\")\n",
    "print(\"You can monitor progress below\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Start training - PASS EXTERNAL DATA TO FIX BUG\n",
    "try:\n",
    "    if USE_EXTERNAL_DATA:\n",
    "        print(f\"âš ï¸ IMPORTANT: Using {len(train_examples)} training examples from external dataset\\n\")\n",
    "        model, tokenizer = fine_tuner.train(\n",
    "            external_train_data=train_examples,\n",
    "            external_val_data=val_examples\n",
    "        )\n",
    "    else:\n",
    "        print(\"âš ï¸ WARNING: Using built-in small dataset (not recommended)\\n\")\n",
    "        model, tokenizer = fine_tuner.train()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Fine-tuning Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total time: {elapsed_time/3600:.2f} hours\")\n",
    "    print(f\"Model saved to: {config.output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  Training interrupted: {str(e)}\")\n",
    "    print(f\"Checkpoints saved in: {config.output_dir}\")\n",
    "    print(\"You can resume training from the last checkpoint\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534cb98",
   "metadata": {
    "papermill": {
     "duration": 0.005918,
     "end_time": "2025-11-11T18:22:32.657289",
     "exception": false,
     "start_time": "2025-11-11T18:22:32.651371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 9. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be52e532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T18:22:32.669621Z",
     "iopub.status.busy": "2025-11-11T18:22:32.669176Z",
     "iopub.status.idle": "2025-11-11T18:22:59.238159Z",
     "shell.execute_reply": "2025-11-11T18:22:59.237228Z"
    },
    "papermill": {
     "duration": 26.576389,
     "end_time": "2025-11-11T18:22:59.239394",
     "exception": false,
     "start_time": "2025-11-11T18:22:32.663005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model for testing...\n",
      "Loading fine-tuned model from /kaggle/working/qwen_irg_finetuned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0420120ad0c94c5cb17d774cc4ab124c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully\n",
      "\n",
      "============================================================\n",
      "Testing Fine-tuned Model\n",
      "============================================================\n",
      "\n",
      "1. Prompt: A serene lake at sunset with mountains in the background\n",
      "------------------------------------------------------------\n",
      "Reasoning: Composition: Use centered to create visual balance and interest.\n",
      "Position the main subject according to this principle.\n",
      "\n",
      "Lighting: Apply soft light to establish mood and depth.\n",
      "Ensure consistent light direction and appropriate shadows.\n",
      "\n",
      "Color palette: Choose hues that support the subject and mood.\n",
      "Apply proper color temperature and saturation.\n",
      "\n",
      "Technical: Maintain sharp focus on the subject with appropriate depth of field.\n",
      "Ensure high detail quality and proper exposure.\n",
      "\n",
      "Apply these principles t...\n",
      "\n",
      "\n",
      "2. Prompt: A cyberpunk street scene with neon lights and rain\n",
      "------------------------------------------------------------\n",
      "Reasoning: Composition: Use frame within frame to create visual balance and interest. Position the main subject according to this principle. Lighting: Apply studio lighting to establish mood and depth. Ensure consistent light direction and appropriate shadows. Color palette: Choose colors that support the subject and mood. Apply proper color temperature and saturation. Technical: Maintain sharp focus on the subject with appropriate depth of field. Ensure high detail quality and proper exposure....\n",
      "\n",
      "\n",
      "3. Prompt: A medieval castle on a hilltop during a thunderstorm\n",
      "------------------------------------------------------------\n",
      "Reasoning: Composition: Use golden ratio to create visual balance and interest. Position the main subject according to this principle.\n",
      "\n",
      "Lighting: Apply blue hour to establish mood and depth. Enhance key details with studious highlights and shadows.\n",
      "\n",
      "Color palette: Choose colors that support the subject and mood. Apply proper color temperature and saturation.\n",
      "\n",
      "Texture: Add realistic textures to natural elements and finishes. Ensure consistent texture application throughout the image.\n",
      "\n",
      "Drawn details: Enhance...\n",
      "\n",
      "\n",
      "4. Prompt: Portrait of an elderly person reading by candlelight\n",
      "------------------------------------------------------------\n",
      "Reasoning: Composition: Use diagonal to create visual balance and interest. Position the main subject according to this principle. Lighting: Apply golden hour to establish mood and depth. Ensure consistent light direction and appropriate shadows. Color palette: Choose colors that support the subject and mood. Apply proper color temperature and saturation. Technical: Maintain sharp focus on the subject with appropriate depth of field. Ensure high detail quality and proper exposure....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from qwen_finetune import OptimizedQwenInference\n",
    "\n",
    "# Load fine-tuned model for testing\n",
    "print(\"Loading fine-tuned model for testing...\")\n",
    "inference = OptimizedQwenInference(\n",
    "    model_path=config.output_dir,\n",
    "    use_lora=config.use_lora\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\\n\")\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"A serene lake at sunset with mountains in the background\",\n",
    "    \"A cyberpunk street scene with neon lights and rain\",\n",
    "    \"A medieval castle on a hilltop during a thunderstorm\",\n",
    "    \"Portrait of an elderly person reading by candlelight\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Fine-tuned Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i}. Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Generate reasoning\n",
    "    reasoning = inference.generate_reasoning(\n",
    "        prompt=prompt,\n",
    "        reasoning_type=\"initial\",\n",
    "        max_length=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"Reasoning: {reasoning[:500]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db39e49",
   "metadata": {
    "papermill": {
     "duration": 0.005941,
     "end_time": "2025-11-11T18:22:59.252010",
     "exception": false,
     "start_time": "2025-11-11T18:22:59.246069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10. Save Model for Next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec0f5f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T18:22:59.264890Z",
     "iopub.status.busy": "2025-11-11T18:22:59.264451Z",
     "iopub.status.idle": "2025-11-11T18:22:59.272487Z",
     "shell.execute_reply": "2025-11-11T18:22:59.271837Z"
    },
    "papermill": {
     "duration": 0.01554,
     "end_time": "2025-11-11T18:22:59.273456",
     "exception": false,
     "start_time": "2025-11-11T18:22:59.257916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files in output directory:\n",
      "============================================================\n",
      "  âœ“ lora_weights/ (directory)\n",
      "  âœ“ added_tokens.json (0.00 MB)\n",
      "  âœ“ checkpoint-600/ (directory)\n",
      "  âœ“ checkpoint-500/ (directory)\n",
      "  âœ“ checkpoint-675/ (directory)\n",
      "  âœ“ adapter_model.safetensors (57.16 MB)\n",
      "  âœ“ tokenizer_config.json (0.00 MB)\n",
      "  âœ“ chat_template.jinja (0.00 MB)\n",
      "  âœ“ tokenizer.json (10.89 MB)\n",
      "  âœ“ special_tokens_map.json (0.00 MB)\n",
      "  âœ“ adapter_config.json (0.00 MB)\n",
      "  âœ“ merges.txt (1.59 MB)\n",
      "  âœ“ README.md (0.01 MB)\n",
      "  âœ“ training_args.bin (0.01 MB)\n",
      "  âœ“ vocab.json (2.65 MB)\n",
      "\n",
      "Total model size: 72.31 MB\n",
      "Location: /kaggle/working/qwen_irg_finetuned\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS:\n",
      "============================================================\n",
      "1. Create a Kaggle Dataset from this output:\n",
      "   - Go to 'File' â†’ 'Download' (or use Kaggle API)\n",
      "   - Create new dataset: 'qwen-irg-finetuned'\n",
      "   - Upload the entire output folder\n",
      "\n",
      "2. Use in Notebook 3 (Benchmarking):\n",
      "   - Add fine-tuned model as input dataset\n",
      "   - Path: /kaggle/input/qwen-irg-finetuned/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Verify model files\n",
    "model_dir = config.output_dir\n",
    "\n",
    "print(\"Model files in output directory:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    for item in os.listdir(model_dir):\n",
    "        item_path = os.path.join(model_dir, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            size_mb = os.path.getsize(item_path) / (1024**2)\n",
    "            print(f\"  âœ“ {item} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"  âœ“ {item}/ (directory)\")\n",
    "    \n",
    "    # Calculate total size\n",
    "    total_size = sum(\n",
    "        os.path.getsize(os.path.join(model_dir, f)) \n",
    "        for f in os.listdir(model_dir) \n",
    "        if os.path.isfile(os.path.join(model_dir, f))\n",
    "    ) / (1024**2)\n",
    "    \n",
    "    print(f\"\\nTotal model size: {total_size:.2f} MB\")\n",
    "    print(f\"Location: {model_dir}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Model directory not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Create a Kaggle Dataset from this output:\")\n",
    "print(\"   - Go to 'File' â†’ 'Download' (or use Kaggle API)\")\n",
    "print(\"   - Create new dataset: 'qwen-irg-finetuned'\")\n",
    "print(\"   - Upload the entire output folder\")\n",
    "print(\"\")\n",
    "print(\"2. Use in Notebook 3 (Benchmarking):\")\n",
    "print(\"   - Add fine-tuned model as input dataset\")\n",
    "print(\"   - Path: /kaggle/input/qwen-irg-finetuned/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dd9db",
   "metadata": {
    "papermill": {
     "duration": 0.007229,
     "end_time": "2025-11-11T18:22:59.286861",
     "exception": false,
     "start_time": "2025-11-11T18:22:59.279632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 11. Optional: Save Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0bb641d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T18:22:59.299660Z",
     "iopub.status.busy": "2025-11-11T18:22:59.299099Z",
     "iopub.status.idle": "2025-11-11T18:22:59.306130Z",
     "shell.execute_reply": "2025-11-11T18:22:59.305507Z"
    },
    "papermill": {
     "duration": 0.014509,
     "end_time": "2025-11-11T18:22:59.307199",
     "exception": false,
     "start_time": "2025-11-11T18:22:59.292690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training logs found\n"
     ]
    }
   ],
   "source": [
    "# If training logs exist, visualize them\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history_file = os.path.join(config.output_dir, \"trainer_state.json\")\n",
    "\n",
    "if os.path.exists(log_history_file):\n",
    "    with open(log_history_file, 'r') as f:\n",
    "        trainer_state = json.load(f)\n",
    "    \n",
    "    if 'log_history' in trainer_state:\n",
    "        logs = pd.DataFrame(trainer_state['log_history'])\n",
    "        \n",
    "        # Plot training curves\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Loss curve\n",
    "        if 'loss' in logs.columns:\n",
    "            axes[0].plot(logs['step'], logs['loss'], label='Training Loss')\n",
    "            axes[0].set_xlabel('Step')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].set_title('Training Loss Over Time')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        if 'learning_rate' in logs.columns:\n",
    "            axes[1].plot(logs['step'], logs['learning_rate'], color='orange')\n",
    "            axes[1].set_xlabel('Step')\n",
    "            axes[1].set_ylabel('Learning Rate')\n",
    "            axes[1].set_title('Learning Rate Schedule')\n",
    "            axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config.output_dir, 'training_curves.png'), dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Training metrics visualized and saved\")\n",
    "else:\n",
    "    print(\"No training logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8cd11",
   "metadata": {
    "papermill": {
     "duration": 0.005964,
     "end_time": "2025-11-11T18:22:59.319056",
     "exception": false,
     "start_time": "2025-11-11T18:22:59.313092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 12. Clean Up GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03bc111e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T18:22:59.331911Z",
     "iopub.status.busy": "2025-11-11T18:22:59.331226Z",
     "iopub.status.idle": "2025-11-11T18:23:00.055350Z",
     "shell.execute_reply": "2025-11-11T18:23:00.054627Z"
    },
    "papermill": {
     "duration": 0.731594,
     "end_time": "2025-11-11T18:23:00.056518",
     "exception": false,
     "start_time": "2025-11-11T18:22:59.324924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU memory cleared\n",
      "GPU 0:\n",
      "  Allocated: 1.18 GB\n",
      "  Reserved: 3.17 GB\n",
      "GPU 1:\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "del tokenizer\n",
    "if 'inference' in locals():\n",
    "    del inference\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ“ GPU memory cleared\")\n",
    "\n",
    "# Show final GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        mem_allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "        mem_reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"  Allocated: {mem_allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {mem_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0b579",
   "metadata": {
    "papermill": {
     "duration": 0.006104,
     "end_time": "2025-11-11T18:23:00.069223",
     "exception": false,
     "start_time": "2025-11-11T18:23:00.063119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## âœ… Fine-tuning Complete!\n",
    "\n",
    "Your fine-tuned Qwen model is ready for the benchmarking phase.\n",
    "\n",
    "**Next:** Create Kaggle dataset from output and move to Notebook 3"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 275860246,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 141449,
     "sourceId": 166236,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 141469,
     "sourceId": 166258,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 164048,
     "modelInstanceId": 141460,
     "sourceId": 166247,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20870.635322,
   "end_time": "2025-11-11T18:23:02.995756",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-11T12:35:12.360434",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0420120ad0c94c5cb17d774cc4ab124c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a09ca455a19b4b36af91aec2dcb1c2bd",
        "IPY_MODEL_d60e4fd6f84142fbbd209c66742d28ed",
        "IPY_MODEL_c9e635d4de6b4cdbabab171edab77c6b"
       ],
       "layout": "IPY_MODEL_073b5578a6ad4f8fabb74fa4b4eab126",
       "tabbable": null,
       "tooltip": null
      }
     },
     "073b5578a6ad4f8fabb74fa4b4eab126": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1dea244fb01b48f19fa9348cf985058e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35ae2527e97c46f7821f1fe04ceda758": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44438d37a042410c94b606511dd249f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "522c57d607f148a5979f30f44b00e4d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "854784e4acdf469ba507556a40f08b72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "88109e4e2b6549d88934274a214e9c70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8970cb81ecc84effad69e1997caa952a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_35ae2527e97c46f7821f1fe04ceda758",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b835866ab5204dae95e564766c87ffb7",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡2/2â€‡[00:56&lt;00:00,â€‡26.58s/it]"
      }
     },
     "9e7bfba524fd46dab5ff7f8089555456": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a09ca455a19b4b36af91aec2dcb1c2bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fb309c9a5fe64c1992ca89531777c3a1",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_9e7bfba524fd46dab5ff7f8089555456",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
      }
     },
     "a7b8de78106e424ea623494d87189024": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "afc4f583996a42c2b6177509c0f7c4d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b835866ab5204dae95e564766c87ffb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c048b67cb3ff46f3a9957154a3b6b5bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d41df1e005a649119f1429c7b763891f",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_854784e4acdf469ba507556a40f08b72",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
      }
     },
     "c9e635d4de6b4cdbabab171edab77c6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1dea244fb01b48f19fa9348cf985058e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a7b8de78106e424ea623494d87189024",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡2/2â€‡[00:02&lt;00:00,â€‡â€‡1.26s/it]"
      }
     },
     "d41df1e005a649119f1429c7b763891f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d60e4fd6f84142fbbd209c66742d28ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_44438d37a042410c94b606511dd249f5",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_88109e4e2b6549d88934274a214e9c70",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "dc97044f56de499db0f560d0bd124b09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c048b67cb3ff46f3a9957154a3b6b5bc",
        "IPY_MODEL_ef766a1d1e2f4ad8a845a83c944a80d3",
        "IPY_MODEL_8970cb81ecc84effad69e1997caa952a"
       ],
       "layout": "IPY_MODEL_ece2924c1ef641f9acf7ee43951f149a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ece2924c1ef641f9acf7ee43951f149a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef766a1d1e2f4ad8a845a83c944a80d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_afc4f583996a42c2b6177509c0f7c4d7",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_522c57d607f148a5979f30f44b00e4d4",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "fb309c9a5fe64c1992ca89531777c3a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
